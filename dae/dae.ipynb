{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b8682b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 2.382969,
     "end_time": "2021-04-03T00:34:23.165613",
     "exception": false,
     "start_time": "2021-04-03T00:34:20.782644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    print('Setting Random Seed')\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed47773",
   "metadata": {
    "papermill": {
     "duration": 0.027921,
     "end_time": "2021-04-03T00:34:23.292331",
     "exception": false,
     "start_time": "2021-04-03T00:34:23.264410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGET = 'service_canceled'\n",
    "\n",
    "fts_continuous = ['customer_age_appl', 'time_start_process', 'operator_count', 'previous_customer_count']\n",
    "fts_categorical = ['date', 'branch_name', 'customer_gender', 'customer_city', 'service_name_organization',\n",
    "                   'service_name', 'service_name_2']\n",
    "\n",
    "print('Categorical Features', fts_categorical)\n",
    "print('Continuous Features', fts_continuous)\n",
    "\n",
    "print('Categorical Feature Count', len(fts_categorical))\n",
    "print('Continuous Feature Count', len(fts_continuous))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99959b88",
   "metadata": {
    "papermill": {
     "duration": 0.018916,
     "end_time": "2021-04-03T00:34:23.331232",
     "exception": false,
     "start_time": "2021-04-03T00:34:23.312316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a9f34",
   "metadata": {
    "papermill": {
     "duration": 0.023552,
     "end_time": "2021-04-03T00:34:23.369398",
     "exception": false,
     "start_time": "2021-04-03T00:34:23.345846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = \"./\"\n",
    "\n",
    "SAVE_PATH = \"./outputs/\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "run_key = 'DAE_TRIAL' #due to long run time prefer to split into sections of 600 epochs\n",
    "\n",
    "DAE_CFG = {'debug': False,\n",
    "           'batch_size': 384, \n",
    "           'init_lr': 0.0003, \n",
    "           'lr_decay': 0.998,  # rate of decrease of learning rate\n",
    "           'noise_decay': 0.999,  # rate of decrease of noise level\n",
    "           'max_epochs': 2000, \n",
    "           'save_freq': 100, \n",
    "           'hidden_size': 1024,  # hidden_size == embed_dim * num_subspaces\n",
    "           'num_subspaces': 8, \n",
    "           'embed_dim': 128, \n",
    "           'num_heads': 8, \n",
    "           'dropout': 0, \n",
    "           'feedforward_dim': 512, \n",
    "           'emphasis': 0.75, #weighing of loss to 'corrupted' data points - i tried varying over time, did not show immediate improvement\n",
    "           'task_weights': [len(fts_categorical), len(fts_continuous)], #weighting for categorical vs continuous\n",
    "           'mask_loss_weight': 2, #weighting of mask prediction vs prediction of reconstructed original data values\n",
    "           'prob_categorical': 0.2, #probability of noise in categoricals\n",
    "           'prob_continuous': 0.1, #probability of noise in continuous\n",
    "           'random_state': 2021,\n",
    "           'run_key': run_key\n",
    "          }\n",
    "\n",
    "if DAE_CFG['debug']:\n",
    "    DAE_CFG['max_epochs'] = 10\n",
    "    \n",
    "with open(SAVE_PATH + f\"{DAE_CFG['run_key']}_DAE_CFG.pickle\", 'wb') as f:\n",
    "    pickle.dump(DAE_CFG, f)\n",
    "with open(SAVE_PATH + f\"{DAE_CFG['run_key']}_DAE_CFG.json\", 'w') as f:\n",
    "    json.dump(DAE_CFG, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345c2cf",
   "metadata": {
    "papermill": {
     "duration": 0.014543,
     "end_time": "2021-04-03T00:34:23.398775",
     "exception": false,
     "start_time": "2021-04-03T00:34:23.384232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Check Noise and Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0409a",
   "metadata": {
    "papermill": {
     "duration": 0.649165,
     "end_time": "2021-04-03T00:34:24.062522",
     "exception": false,
     "start_time": "2021-04-03T00:34:23.413357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tracking_df = pd.DataFrame(index=range(DAE_CFG['max_epochs']),\n",
    "                           columns=['loss', 'lr', 'run_code', 'time', 'elapsed', 'noise_categorical', 'noise_continuous'],\n",
    "                           data=0.0)\n",
    "\n",
    "tracking_df['lr'] = DAE_CFG['init_lr'] * (DAE_CFG['lr_decay']**tracking_df.index)\n",
    "tracking_df['noise_categorical'] = DAE_CFG['prob_categorical'] * (DAE_CFG['noise_decay']**tracking_df.index)\n",
    "tracking_df['noise_continuous'] = DAE_CFG['prob_continuous'] * (DAE_CFG['noise_decay']**tracking_df.index)\n",
    "tracking_df['run_code'] = DAE_CFG['run_key']\n",
    "\n",
    "sns.set(font_scale=1.4)\n",
    "fig,axes=plt.subplots(nrows=1,ncols=3,figsize=(18,6))\n",
    "\n",
    "axes[0].plot(tracking_df.index, tracking_df['lr'], color='Blue')\n",
    "axes[0].set_ylim(0,)\n",
    "axes[0].set_title('Learning Rate')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "\n",
    "axes[1].plot(tracking_df.index, tracking_df['noise_categorical'], color='Red')\n",
    "axes[1].set_ylim(0,1)\n",
    "axes[1].set_title('Categorical Noise Prob')\n",
    "\n",
    "axes[2].plot(tracking_df.index, tracking_df['noise_continuous'], color='Red')\n",
    "axes[2].set_ylim(0,1)\n",
    "axes[2].set_title('Continuous Noise Prob')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b26478c",
   "metadata": {
    "papermill": {
     "duration": 0.017143,
     "end_time": "2021-04-03T00:34:24.098293",
     "exception": false,
     "start_time": "2021-04-03T00:34:24.081150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Functions to Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feats_engineering(train, test):\n",
    "    unique_counts = list()\n",
    "    all_df = pd.concat([train, test]).reset_index(drop=True)\n",
    "    \n",
    "    all_df['customer_age_appl'].fillna(all_df['customer_age_appl'].mode()[0], inplace=True)\n",
    "    all_df['time_start_process'].fillna(all_df['time_start_process'].mode()[0], inplace=True)\n",
    "\n",
    "    all_df['customer_age_appl'] = all_df['customer_age_appl'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2)\n",
    "    all_df['time_start_process'] = all_df['time_start_process'].apply(lambda x: int(x[:2]))\n",
    "\n",
    "    for col in fts_categorical:\n",
    "        unique_counts.append(all_df[col].nunique())\n",
    "        \n",
    "    df_train = all_df[:train.shape[0]]\n",
    "    df_test = all_df[train.shape[0]:].reset_index(drop=True)\n",
    "        \n",
    "    return df_train, df_test, unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3250d5",
   "metadata": {
    "papermill": {
     "duration": 0.028893,
     "end_time": "2021-04-03T00:34:24.143964",
     "exception": false,
     "start_time": "2021-04-03T00:34:24.115071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train = pd.read_csv(PATH + \"queue_dataset_train_small_sample.csv\")\n",
    "    test = pd.read_csv(PATH + \"queue_dataset_test.csv\")\n",
    "    \n",
    "    train_data, test_data, unique_counts = feats_engineering(train, test)\n",
    "    \n",
    "    #combine train and test data vertically\n",
    "    X_nums = np.vstack([\n",
    "        train_data.loc[:, fts_continuous].to_numpy(),\n",
    "        test_data.loc[:, fts_continuous].to_numpy()\n",
    "    ])\n",
    "    X_nums = (X_nums - X_nums.mean(0)) / X_nums.std(0) #normalize\n",
    "    \n",
    "    #stack the categorical data\n",
    "    X_cat = np.vstack([\n",
    "        train_data.loc[:, fts_categorical].to_numpy(),\n",
    "        test_data.loc[:, fts_categorical].to_numpy()\n",
    "    ])\n",
    "    #encode the categoricals\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    X_cat = encoder.fit_transform(X_cat)\n",
    "    \n",
    "    #join the categorical and continuous data horizontally\n",
    "    X = np.hstack([X_cat, X_nums])\n",
    "    y = train_data[TARGET].to_numpy().reshape(-1, 1)\n",
    "    \n",
    "    return X, y, X_cat.shape[1], X_nums.shape[1], unique_counts #this lets us know how many categorical and continuous features there are\n",
    "\n",
    "class SingleDataset(Dataset):\n",
    "    def __init__(self, x, is_sparse=False):\n",
    "        self.x = x.astype('float32')\n",
    "        self.is_sparse = is_sparse\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        if self.is_sparse: x = x.toarray().squeeze()\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c748d87",
   "metadata": {
    "papermill": {
     "duration": 0.016393,
     "end_time": "2021-04-03T00:34:24.176806",
     "exception": false,
     "start_time": "2021-04-03T00:34:24.160413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875fa794",
   "metadata": {
    "papermill": {
     "duration": 0.023675,
     "end_time": "2021-04-03T00:34:24.216957",
     "exception": false,
     "start_time": "2021-04-03T00:34:24.193282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bce_logits = torch.nn.functional.binary_cross_entropy_with_logits\n",
    "mse = torch.nn.functional.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478551e9",
   "metadata": {
    "papermill": {
     "duration": 0.017758,
     "end_time": "2021-04-03T00:34:24.251735",
     "exception": false,
     "start_time": "2021-04-03T00:34:24.233977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5907047e",
   "metadata": {
    "papermill": {
     "duration": 0.029315,
     "end_time": "2021-04-03T00:34:24.299261",
     "exception": false,
     "start_time": "2021-04-03T00:34:24.269946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#torch docs\n",
    "\n",
    "#embed_dim – total dimension of the model.\n",
    "#num_heads – parallel attention heads.\n",
    "#dropout – a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "#bias – add bias as module parameter. Default: True.\n",
    "#add_bias_kv – add bias to the key and value sequences at dim=0.\n",
    "#add_zero_attn – add a new batch of zeros to the key and value sequences at dim=1.\n",
    "#kdim – total number of features in key. Default: None.\n",
    "#vdim – total number of features in value. Default: None.\n",
    "\n",
    "class TransformerEncoder(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, feedforward_dim):\n",
    "        super().__init__()\n",
    "        self.attn = torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.linear_1 = torch.nn.Linear(embed_dim, feedforward_dim)\n",
    "        self.linear_2 = torch.nn.Linear(feedforward_dim, embed_dim)\n",
    "        self.layernorm_1 = torch.nn.LayerNorm(embed_dim)\n",
    "        self.layernorm_2 = torch.nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x_in):        \n",
    "        attn_out, _ = self.attn(x_in, x_in, x_in)        \n",
    "        x = self.layernorm_1(x_in + attn_out)\n",
    "        ff_out = self.linear_2(torch.nn.functional.relu(self.linear_1(x)))\n",
    "        x = self.layernorm_2(x + ff_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d1e39",
   "metadata": {
    "papermill": {
     "duration": 0.037535,
     "end_time": "2021-04-03T00:34:24.358466",
     "exception": false,
     "start_time": "2021-04-03T00:34:24.320931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerAutoEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_inputs, \n",
    "            n_cats, \n",
    "            n_nums, \n",
    "            hidden_size=1024, \n",
    "            num_subspaces=8,\n",
    "            embed_dim=128, \n",
    "            num_heads=8, \n",
    "            dropout=0, \n",
    "            feedforward_dim=512, \n",
    "            emphasis=.75, \n",
    "            task_weights=[len(fts_categorical), len(fts_continuous)],\n",
    "            mask_loss_weight=2,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert hidden_size == embed_dim * num_subspaces\n",
    "        self.n_cats = n_cats\n",
    "        self.n_nums = n_nums\n",
    "        self.num_subspaces = num_subspaces\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.emphasis = emphasis\n",
    "        self.task_weights = np.array(task_weights) / sum(task_weights)\n",
    "        self.mask_loss_weight = mask_loss_weight\n",
    "\n",
    "        self.excite = torch.nn.Linear(in_features=num_inputs, out_features=hidden_size)\n",
    "        self.encoder_1 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n",
    "        self.encoder_2 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n",
    "        self.encoder_3 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)        \n",
    "        \n",
    "        self.mask_predictor = torch.nn.Linear(in_features=hidden_size, out_features=num_inputs)\n",
    "        self.reconstructor = torch.nn.Linear(in_features=hidden_size + num_inputs, out_features=num_inputs)\n",
    "\n",
    "    def divide(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape((batch_size, self.num_subspaces, self.embed_dim)).permute((1, 0, 2))\n",
    "        return x\n",
    "\n",
    "    def combine(self, x):\n",
    "        batch_size = x.shape[1]\n",
    "        x = x.permute((1, 0, 2)).reshape((batch_size, -1))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.excite(x))\n",
    "        \n",
    "        x = self.divide(x)\n",
    "        x1 = self.encoder_1(x)\n",
    "        x2 = self.encoder_2(x1)\n",
    "        x3 = self.encoder_3(x2)\n",
    "        x = self.combine(x3)\n",
    "        \n",
    "        predicted_mask = self.mask_predictor(x)\n",
    "        reconstruction = self.reconstructor(torch.cat([x, predicted_mask], dim=1))\n",
    "        return (x1, x2, x3), (reconstruction, predicted_mask)\n",
    "\n",
    "    def split(self, t):\n",
    "        return torch.split(t, [self.n_cats, self.n_nums], dim=1)\n",
    "\n",
    "    def feature(self, x):\n",
    "        attn_outs, _ = self.forward(x)\n",
    "        return torch.cat([self.combine(x) for x in attn_outs], dim=1) #the feature is the data extracted for use in inference\n",
    "\n",
    "    def loss(self, x, y, mask, reduction='mean'):   \n",
    "        _, (reconstruction, predicted_mask) = self.forward(x)\n",
    "        \n",
    "        x_cats, x_nums = self.split(reconstruction)\n",
    "        y_cats, y_nums = self.split(y)\n",
    "        \n",
    "        #weights are detemined by the emphasis - which is currently heavier weights for corrupted data (mask = 1)\n",
    "        w_cats, w_nums = self.split(mask * self.emphasis + (1 - mask) * (1 - self.emphasis))\n",
    "        \n",
    "        #BCE loss for reconstructed vs actual categoricals\n",
    "        cat_loss = self.task_weights[0] * torch.mul(w_cats, bce_logits(x_cats, y_cats, reduction='none')) \n",
    "        \n",
    "        #mse loss for reconstructed vs actual continuous\n",
    "        num_loss = self.task_weights[1] * torch.mul(w_nums, mse(x_nums, y_nums, reduction='none'))\n",
    "        \n",
    "        #BCE+MSE = reconstruction loss\n",
    "        reconstruction_loss = torch.cat([cat_loss, num_loss], dim=1) if reduction == 'none' else cat_loss.mean() + num_loss.mean()\n",
    "        \n",
    "        #mask loss = how well the model predicts which values are corrupted - can use BCE as this is 0/1\n",
    "        mask_loss = self.mask_loss_weight * bce_logits(predicted_mask, mask, reduction=reduction)\n",
    "        \n",
    "        return reconstruction_loss + mask_loss if reduction == 'mean' else [reconstruction_loss, mask_loss]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eda3c2",
   "metadata": {
    "papermill": {
     "duration": 0.016999,
     "end_time": "2021-04-03T00:34:24.392120",
     "exception": false,
     "start_time": "2021-04-03T00:34:24.375121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Noise Masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a72dcc",
   "metadata": {
    "papermill": {
     "duration": 0.025632,
     "end_time": "2021-04-03T00:34:24.434534",
     "exception": false,
     "start_time": "2021-04-03T00:34:24.408902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SwapNoiseMasker(object):\n",
    "    def __init__(self, probas, decay_rate):\n",
    "        self.probas = torch.from_numpy(np.array(probas))\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "    def apply(self, X, epoch_number):\n",
    "        epoch_probas = self.probas * (self.decay_rate ** epoch_number)\n",
    "        \n",
    "        #generates Y/N for swap / dont swap\n",
    "        should_swap = torch.bernoulli(epoch_probas.to(X.device) * torch.ones((X.shape)).to(X.device))\n",
    "        \n",
    "        #switches data where swap = Y\n",
    "        corrupted_X = torch.where(should_swap == 1, X[torch.randperm(X.shape[0])], X)\n",
    "        \n",
    "        #mask is whether data has been changed or not\n",
    "        #nb for one hot categorical data, presumably quite often mask != shouldswap, as 1 is swapped for 1 or 0 swapped for 0\n",
    "        mask = (corrupted_X != X).float()\n",
    "        return corrupted_X, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0685eb2",
   "metadata": {
    "papermill": {
     "duration": 0.017055,
     "end_time": "2021-04-03T00:34:24.579827",
     "exception": false,
     "start_time": "2021-04-03T00:34:24.562772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1269454b",
   "metadata": {
    "papermill": {
     "duration": 11.053494,
     "end_time": "2021-04-03T00:34:35.650869",
     "exception": false,
     "start_time": "2021-04-03T00:34:24.597375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  get data\n",
    "X, Y, n_cats, n_nums, unique_counts = get_data()\n",
    "\n",
    "seed_everything(DAE_CFG['random_state'])\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    dataset=SingleDataset(X),\n",
    "    batch_size=DAE_CFG['batch_size'],\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(X.shape, Y.shape, n_cats, n_nums)\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8039206",
   "metadata": {
    "papermill": {
     "duration": 0.016657,
     "end_time": "2021-04-03T00:34:24.468039",
     "exception": false,
     "start_time": "2021-04-03T00:34:24.451382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define Column Noise Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756c54f",
   "metadata": {
    "papermill": {
     "duration": 0.027148,
     "end_time": "2021-04-03T00:34:24.545366",
     "exception": false,
     "start_time": "2021-04-03T00:34:24.518218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#repeats of probabilities for one hot encoding which creates new columns for categoricals\n",
    "repeats = [x for x in unique_counts] + [1 for x in range(len(fts_continuous))]\n",
    "\n",
    "#probabilities for original columns\n",
    "probas = [DAE_CFG['prob_categorical'] for x in range(len(fts_categorical))] + [DAE_CFG['prob_continuous'] for x in range(len(fts_continuous))]\n",
    "\n",
    "#expand these to the one hot columns\n",
    "swap_probas = sum([[p] * r for p, r in zip(probas, repeats)], [])\n",
    "\n",
    "print('length', len(swap_probas))\n",
    "print('examples', swap_probas[0:10], swap_probas[-len(fts_continuous):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb101e96",
   "metadata": {
    "papermill": {
     "duration": 0.017949,
     "end_time": "2021-04-03T00:34:35.687107",
     "exception": false,
     "start_time": "2021-04-03T00:34:35.669158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare DAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4857120",
   "metadata": {
    "papermill": {
     "duration": 4.341149,
     "end_time": "2021-04-03T00:34:40.046160",
     "exception": false,
     "start_time": "2021-04-03T00:34:35.705011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup model\n",
    "\n",
    "model_params = dict(\n",
    "    hidden_size=DAE_CFG['hidden_size'],\n",
    "    num_subspaces=DAE_CFG['num_subspaces'],\n",
    "    embed_dim=DAE_CFG['embed_dim'],\n",
    "    num_heads=DAE_CFG['num_heads'],\n",
    "    dropout=DAE_CFG['dropout'],\n",
    "    feedforward_dim=DAE_CFG['feedforward_dim'],\n",
    "    emphasis=DAE_CFG['emphasis'],\n",
    "    mask_loss_weight=DAE_CFG['mask_loss_weight']\n",
    ")\n",
    "\n",
    "dae = TransformerAutoEncoder(\n",
    "    num_inputs=X.shape[1],\n",
    "    n_cats=n_cats,\n",
    "    n_nums=n_nums,\n",
    "    **model_params\n",
    ").cuda()\n",
    "model_checkpoint = 'model_checkpoint.pth'\n",
    "\n",
    "optimizer = torch.optim.Adam(dae.parameters(), lr=DAE_CFG['init_lr'])\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=DAE_CFG['lr_decay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e6665",
   "metadata": {
    "papermill": {
     "duration": 0.017748,
     "end_time": "2021-04-03T00:34:40.083387",
     "exception": false,
     "start_time": "2021-04-03T00:34:40.065639",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training DAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea6559b",
   "metadata": {
    "papermill": {
     "duration": 0.026684,
     "end_time": "2021-04-03T00:34:40.128098",
     "exception": false,
     "start_time": "2021-04-03T00:34:40.101414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a5719",
   "metadata": {
    "papermill": {
     "duration": 12747.024646,
     "end_time": "2021-04-03T04:07:07.170575",
     "exception": false,
     "start_time": "2021-04-03T00:34:40.145929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise_maker = SwapNoiseMasker(swap_probas, DAE_CFG['noise_decay'])\n",
    "s0 = time.time()\n",
    "\n",
    "for epoch in range(DAE_CFG['max_epochs']):    \n",
    "       \n",
    "    t0 = time.time()\n",
    "    dae.train()\n",
    "    meter = AverageMeter()\n",
    "    \n",
    "    for i, x in enumerate(train_dl):\n",
    "        x = x.cuda()\n",
    "        x_corrputed, mask = noise_maker.apply(x, epoch) #added epoch to allow noise level to decrease over time\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = dae.loss(x_corrputed, x, mask)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        meter.update(loss.detach().cpu().numpy())\n",
    "\n",
    "    delta1 = (time.time() - s0)\n",
    "    delta2 = (time.time() - t0)\n",
    "    scheduler.step()\n",
    "    \n",
    "    remain = (DAE_CFG['max_epochs'] - (epoch+1)) * delta2\n",
    "    print(f\"\\r epoch {epoch:5d} - loss {meter.avg:.6f} - {delta2:4.2f} sec per epoch - {delta1:6.2f} sec elapsed - {remain:6.2f} sec remaining\", end=\"\")  \n",
    "    \n",
    "    model_checkpoint = f'model_checkpoint_{epoch}.pth'\n",
    "\n",
    "    tracking_df.loc[epoch, 'loss'] = meter.avg\n",
    "    tracking_df.loc[epoch, 'time'] = round(delta2, 2)\n",
    "    tracking_df.loc[epoch, 'elapsed'] = round(delta1, 2)\n",
    "    \n",
    "    if epoch%DAE_CFG['save_freq']==0:\n",
    "        \n",
    "        ## print('Saving to checkpoint')\n",
    "        #as i have flat noise level across all columns, i just print the noise average\n",
    "        ## print('average noise level', np.array(swap_probas).mean()*(DAE_CFG['noise_decay']**epoch))\n",
    "        model_checkpoint = SAVE_PATH + f\"{DAE_CFG['run_key']}_model_checkpoint_{epoch}.pth\"\n",
    "        torch.save({\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "                \"model\": dae.state_dict()\n",
    "            }, model_checkpoint\n",
    "        )\n",
    "        \n",
    "\n",
    "model_checkpoint = SAVE_PATH + f\"{DAE_CFG['run_key']}_model_checkpoint_final.pth\"\n",
    "torch.save({\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict(),\n",
    "        \"model\": dae.state_dict()\n",
    "    }, model_checkpoint\n",
    ")\n",
    "\n",
    "tracking_df.to_csv(f\"{DAE_CFG['run_key']}_tracking_loss.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73196b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "dl = DataLoader(dataset=SingleDataset(X), batch_size=DAE_CFG['batch_size'], shuffle=False, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab2ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "dae.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in dl:\n",
    "        features.append(dae.feature(x.cuda()).detach().cpu().numpy())\n",
    "features = np.vstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c092d34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(features.shape)\n",
    "np.save(SAVE_PATH + f\"{DAE_CFG['run_key']}_dae_features_epoch{DAE_CFG['max_epochs']}.npy\", features)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca3b4a2f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12773.507323,
   "end_time": "2021-04-03T04:07:09.198570",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-03T00:34:15.691247",
   "version": "2.3.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}