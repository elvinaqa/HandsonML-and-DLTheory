{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb79ec",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 2.460223,
     "end_time": "2021-04-04T11:45:56.365132",
     "exception": false,
     "start_time": "2021-04-04T11:45:53.904909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf3721",
   "metadata": {
    "papermill": {
     "duration": 0.023655,
     "end_time": "2021-04-04T11:45:56.493968",
     "exception": false,
     "start_time": "2021-04-04T11:45:56.470313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    print('Setting Random Seed')\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac832a84",
   "metadata": {
    "papermill": {
     "duration": 0.014812,
     "end_time": "2021-04-04T11:45:56.523881",
     "exception": false,
     "start_time": "2021-04-04T11:45:56.509069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46637f0e",
   "metadata": {
    "papermill": {
     "duration": 0.014942,
     "end_time": "2021-04-04T11:45:56.553862",
     "exception": false,
     "start_time": "2021-04-04T11:45:56.538920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The weights used in this notebook (VERSION 5) are from a run with the best settings I managed to develop during the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493b1701",
   "metadata": {
    "papermill": {
     "duration": 0.023818,
     "end_time": "2021-04-04T11:45:56.593270",
     "exception": false,
     "start_time": "2021-04-04T11:45:56.569452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = \"./\"\n",
    "SAVE_PATH = \"./outputs/\"\n",
    "\n",
    "run_key = 'DAE_TRIAL'\n",
    "\n",
    "CFG = {'debug': False,\n",
    "       'debug_epochs': 2,\n",
    "       'model_name': 'mlp',\n",
    "       'lr' : 1e-04,\n",
    "       'weight_decay': 9.72918866945795E-06,\n",
    "       'epochs': 10,\n",
    "       'device': 'cuda',\n",
    "       'nfolds': 10,       \n",
    "       'mlp_hidden_size': 391,\n",
    "       'mlp_size_decline' : 1.0,\n",
    "       'mlp_batch_size': 512,\n",
    "       'mlp_dropout': 0.3,\n",
    "       'bce_smooth' : 0.0001,\n",
    "       'target_dae' : SAVE_PATH,\n",
    "       'target_epoch' : f'{run_key}_model_checkpoint_600.pth',\n",
    "       'random_state': 2021,\n",
    "       'mlp_start_noise' : 0.15,\n",
    "       'mlp_noise_decay' : 0.65,\n",
    "       'run_key': run_key\n",
    "       }\n",
    "\n",
    "if CFG['debug']:\n",
    "    CFG['epochs'] = CFG['debug_epochs']\n",
    "\n",
    "with open(SAVE_PATH + f\"{CFG['run_key']}_CFG.pickle\", 'wb') as f:\n",
    "    pickle.dump(CFG, f)\n",
    "with open(SAVE_PATH + f\"{CFG['run_key']}_CFG.json\", 'w') as f:\n",
    "    json.dump(CFG, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17640133",
   "metadata": {
    "papermill": {
     "duration": 0.165907,
     "end_time": "2021-04-04T11:45:56.774416",
     "exception": false,
     "start_time": "2021-04-04T11:45:56.608509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot noise during MLP training\n",
    "plt.plot(range(CFG['epochs']),\n",
    "         np.array([CFG['mlp_start_noise']*(CFG['mlp_noise_decay']**e) for e in range(CFG['epochs'])]))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Noise During Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b29f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dae_features = glob.glob(SAVE_PATH + f\"{CFG['run_key']}_dae_features*.npy\")\n",
    "num_dae_features = np.load(dae_features[0]).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a779d99",
   "metadata": {
    "papermill": {
     "duration": 0.027652,
     "end_time": "2021-04-04T11:45:56.818897",
     "exception": false,
     "start_time": "2021-04-04T11:45:56.791245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGET = 'service_canceled'\n",
    "\n",
    "fts_continuous = ['customer_age_appl', 'time_start_process', 'operator_count', 'previous_customer_count']\n",
    "fts_categorical = ['date', 'branch_name', 'customer_gender', 'customer_city', 'service_name_organization',\n",
    "                   'service_name', 'service_name_2']\n",
    "\n",
    "# unique counts should be the count of train PLUS test\n",
    "# unique_counts = [4, 3, 3, 2, 2, 2, 334, 2, 408, 3, 726, 18, 12, 12, 13, 6, 31, 2]\n",
    "\n",
    "print('Categorical Features', fts_categorical)\n",
    "print('Continuous Features', fts_continuous)\n",
    "\n",
    "print('Categorical Feature Count', len(fts_categorical))\n",
    "print('Continuous Feature Count', len(fts_continuous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d82fe",
   "metadata": {
    "papermill": {
     "duration": 0.025645,
     "end_time": "2021-04-04T11:45:56.861984",
     "exception": false,
     "start_time": "2021-04-04T11:45:56.836339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#this is just to control settings for the DAE hidden size etc, there is no further training of the DAE in this notebook\n",
    "\n",
    "DAE_CFG = {'debug': False,\n",
    "           'batch_size': 384, \n",
    "           'init_lr': 0.0003, \n",
    "           'lr_decay': 0.998,  # rate of decrease of learning rate\n",
    "           'noise_decay': 0.999,  # rate of decrease of noise level\n",
    "           'max_epochs': 2000, \n",
    "           'save_freq': 100, \n",
    "           'hidden_size': 1024,  # hidden_size == embed_dim * num_subspaces\n",
    "           'num_subspaces': 8, \n",
    "           'embed_dim': 128, \n",
    "           'num_heads': 8, \n",
    "           'dropout': 0, \n",
    "           'feedforward_dim': 512, \n",
    "           'emphasis': 0.75, #weighing of loss to 'corrupted' data points - i tried varying over time, did not show immediate improvement\n",
    "           'task_weights': [len(fts_categorical), len(fts_continuous)], #weighting for categorical vs continuous\n",
    "           'mask_loss_weight': 2, #weighting of mask prediction vs prediction of reconstructed original data values\n",
    "           'prob_categorical': 0.2, #probability of noise in categoricals\n",
    "           'prob_continuous': 0.1, #probability of noise in continuous\n",
    "           'random_state': 2021,\n",
    "           'run_key': run_key\n",
    "          }\n",
    "\n",
    "model_params = dict(\n",
    "    hidden_size=DAE_CFG['hidden_size'],\n",
    "    num_subspaces=DAE_CFG['num_subspaces'],\n",
    "    embed_dim=DAE_CFG['embed_dim'],\n",
    "    num_heads=DAE_CFG['num_heads'],\n",
    "    dropout=DAE_CFG['dropout'],\n",
    "    feedforward_dim=DAE_CFG['feedforward_dim'],\n",
    "    emphasis=DAE_CFG['emphasis'],\n",
    "    mask_loss_weight=DAE_CFG['mask_loss_weight']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb459db5",
   "metadata": {
    "papermill": {
     "duration": 0.017007,
     "end_time": "2021-04-04T11:45:56.896041",
     "exception": false,
     "start_time": "2021-04-04T11:45:56.879034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DAE Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feats_engineering(train, test):\n",
    "    unique_counts = list()\n",
    "    all_df = pd.concat([train, test]).reset_index(drop=True)\n",
    "    \n",
    "    all_df['customer_age_appl'].fillna(all_df['customer_age_appl'].mode()[0], inplace=True)\n",
    "    all_df['time_start_process'].fillna(all_df['time_start_process'].mode()[0], inplace=True)\n",
    "\n",
    "    all_df['customer_age_appl'] = all_df['customer_age_appl'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2)\n",
    "    all_df['time_start_process'] = all_df['time_start_process'].apply(lambda x: int(x[:2]))\n",
    "\n",
    "    for col in fts_categorical:\n",
    "        unique_counts.append(all_df[col].nunique())\n",
    "        \n",
    "    df_train = all_df[:train.shape[0]]\n",
    "    df_test = all_df[train.shape[0]:].reset_index(drop=True)\n",
    "        \n",
    "    return df_train, df_test, unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270ca5a",
   "metadata": {
    "papermill": {
     "duration": 0.030386,
     "end_time": "2021-04-04T11:45:56.943341",
     "exception": false,
     "start_time": "2021-04-04T11:45:56.912955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def get_data():\n",
    "    train = pd.read_csv(PATH + \"queue_dataset_train_small_sample.csv\")\n",
    "    test = pd.read_csv(PATH + \"queue_dataset_test.csv\")\n",
    "    \n",
    "    train_data, test_data, unique_counts = feats_engineering(train, test)\n",
    "    \n",
    "    #combine train and test data vertically\n",
    "    X_nums = np.vstack([\n",
    "        train_data.loc[:, fts_continuous].to_numpy(),\n",
    "        test_data.loc[:, fts_continuous].to_numpy()\n",
    "    ])\n",
    "    X_nums = (X_nums - X_nums.mean(0)) / X_nums.std(0) #normalize\n",
    "    \n",
    "    #stack the categorical data\n",
    "    X_cat = np.vstack([\n",
    "        train_data.loc[:, fts_categorical].to_numpy(),\n",
    "        test_data.loc[:, fts_categorical].to_numpy()\n",
    "    ])\n",
    "    #encode the categoricals\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    X_cat = encoder.fit_transform(X_cat)\n",
    "    \n",
    "    #join the categorical and continuous data horizontally\n",
    "    X = np.hstack([X_cat, X_nums])\n",
    "    y = train_data[TARGET].to_numpy().reshape(-1, 1)\n",
    "    \n",
    "    return X, y, X_cat.shape[1], X_nums.shape[1], unique_counts #this lets us know how many categorical and continuous features there are\n",
    "\n",
    "\n",
    "class FeatureDataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "    \n",
    "class TestFeatureDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),   \n",
    "        }\n",
    "        return dct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de7557b",
   "metadata": {
    "papermill": {
     "duration": 0.042834,
     "end_time": "2021-04-04T11:45:57.002967",
     "exception": false,
     "start_time": "2021-04-04T11:45:56.960133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bce_logits = torch.nn.functional.binary_cross_entropy_with_logits\n",
    "mse = torch.nn.functional.mse_loss\n",
    "\n",
    "class TransformerEncoder(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, feedforward_dim):\n",
    "        super().__init__()\n",
    "        self.attn = torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.linear_1 = torch.nn.Linear(embed_dim, feedforward_dim)\n",
    "        self.linear_2 = torch.nn.Linear(feedforward_dim, embed_dim)\n",
    "        self.layernorm_1 = torch.nn.LayerNorm(embed_dim)\n",
    "        self.layernorm_2 = torch.nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        attn_out, _ = self.attn(x_in, x_in, x_in)\n",
    "        x = self.layernorm_1(x_in + attn_out)\n",
    "        ff_out = self.linear_2(torch.nn.functional.relu(self.linear_1(x)))\n",
    "        x = self.layernorm_2(x + ff_out)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerAutoEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_inputs, \n",
    "            n_cats, \n",
    "            n_nums, \n",
    "            hidden_size=1024, \n",
    "            num_subspaces=8,\n",
    "            embed_dim=128, \n",
    "            num_heads=8, \n",
    "            dropout=0, \n",
    "            feedforward_dim=512, \n",
    "            emphasis=.75, \n",
    "            task_weights=[len(fts_categorical), len(fts_continuous)],\n",
    "            mask_loss_weight=2,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert hidden_size == embed_dim * num_subspaces\n",
    "        self.n_cats = n_cats\n",
    "        self.n_nums = n_nums\n",
    "        self.num_subspaces = num_subspaces\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.emphasis = emphasis\n",
    "        self.task_weights = np.array(task_weights) / sum(task_weights)\n",
    "        self.mask_loss_weight = mask_loss_weight\n",
    "\n",
    "        self.excite = torch.nn.Linear(in_features=num_inputs, out_features=hidden_size)\n",
    "        self.encoder_1 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n",
    "        self.encoder_2 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n",
    "        self.encoder_3 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n",
    "        \n",
    "        \n",
    "        self.mask_predictor = torch.nn.Linear(in_features=hidden_size, out_features=num_inputs)\n",
    "        self.reconstructor = torch.nn.Linear(in_features=hidden_size + num_inputs, out_features=num_inputs)\n",
    "\n",
    "    def divide(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape((batch_size, self.num_subspaces, self.embed_dim)).permute((1, 0, 2))\n",
    "        return x\n",
    "\n",
    "    def combine(self, x):\n",
    "        batch_size = x.shape[1]\n",
    "        x = x.permute((1, 0, 2)).reshape((batch_size, -1))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.excite(x))\n",
    "        \n",
    "        x = self.divide(x)\n",
    "        x1 = self.encoder_1(x)\n",
    "        x2 = self.encoder_2(x1)\n",
    "        x3 = self.encoder_3(x2)\n",
    "        x = self.combine(x3)\n",
    "        \n",
    "        predicted_mask = self.mask_predictor(x)\n",
    "        reconstruction = self.reconstructor(torch.cat([x, predicted_mask], dim=1))\n",
    "        return (x1, x2, x3), (reconstruction, predicted_mask)\n",
    "\n",
    "    def split(self, t):\n",
    "        return torch.split(t, [self.n_cats, self.n_nums], dim=1)\n",
    "\n",
    "    #def feature(self, x):\n",
    "        #attn_outs, _ = self.forward(x)\n",
    "        #return torch.cat([self.combine(x) for x in attn_outs], dim=1)\n",
    "    \n",
    "    #i have modified the feature output to include the reconstruction / mask outputs\n",
    "    #this needs checking in more detail - think range of values may be different\n",
    "    def feature(self, x):\n",
    "        #this returns the autoencoder layer outputs as a concatenated feature set\n",
    "        attn_outs, _ = self.forward(x)\n",
    "        attn_outs = torch.cat([self.combine(x) for x in attn_outs], dim=1)\n",
    "        masks = torch.cat([x for x in _], dim=1)\n",
    "        return torch.cat([attn_outs, masks], dim=1)\n",
    "\n",
    "    def loss(self, x, y, mask, reduction='mean'):        \n",
    "        _, (reconstruction, predicted_mask) = self.forward(x)\n",
    "        \n",
    "        x_cats, x_nums = self.split(reconstruction)\n",
    "        y_cats, y_nums = self.split(y)\n",
    "        w_cats, w_nums = self.split(mask * self.emphasis + (1 - mask) * (1 - self.emphasis))\n",
    "        cat_loss = self.task_weights[0] * torch.mul(w_cats, bce_logits(x_cats, y_cats, reduction='none'))\n",
    "        num_loss = self.task_weights[1] * torch.mul(w_nums, mse(x_nums, y_nums, reduction='none'))\n",
    "        \n",
    "        reconstruction_loss = torch.cat([cat_loss, num_loss], dim=1) if reduction == 'none' else cat_loss.mean() + num_loss.mean()\n",
    "        \n",
    "        mask_loss = self.mask_loss_weight * bce_logits(predicted_mask, mask, reduction=reduction)\n",
    "\n",
    "        return reconstruction_loss + mask_loss if reduction == 'mean' else [reconstruction_loss, mask_loss]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7562251f",
   "metadata": {
    "papermill": {
     "duration": 0.028084,
     "end_time": "2021-04-04T11:45:57.048532",
     "exception": false,
     "start_time": "2021-04-04T11:45:57.020448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SwapNoiseMasker(object):\n",
    "    def __init__(self, probas):\n",
    "        self.probas = torch.from_numpy(np.array(probas))\n",
    "\n",
    "    def apply(self, X):\n",
    "        #provides a distribution of points where we want to corrupt the data        \n",
    "        should_swap = torch.bernoulli(self.probas.to(X.device) * torch.ones((X.shape)).to(X.device))\n",
    "        \n",
    "        #provides a corruped X output\n",
    "        corrupted_X = torch.where(should_swap == 1, X[torch.randperm(X.shape[0])], X)\n",
    "        \n",
    "        #calculates the mask which we aim to predict\n",
    "        mask = (corrupted_X != X).float()\n",
    "        return corrupted_X, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0717d7",
   "metadata": {
    "papermill": {
     "duration": 0.017484,
     "end_time": "2021-04-04T11:45:57.083656",
     "exception": false,
     "start_time": "2021-04-04T11:45:57.066172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b830bf-7733-4216-b914-a596f6bd6458",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features=3000, num_targets=1, hidden_size=1000):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(CFG['mlp_dropout'])\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(CFG['mlp_dropout'])\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(CFG['mlp_dropout'])\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06beedb-61ae-4814-9890-9d379910a057",
   "metadata": {},
   "source": [
    "# Smoothed BCE Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1826a686-7ba3-4e5b-86b8-382b9c5785f2",
   "metadata": {},
   "source": [
    "Note: I don't observe a huge benefit from smoothing - maybe a small gain from a very small smoothing amount. Just used this to have the option. There are some good discussions / notebooks on label smoothing elsewhere on Kaggle if not familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f0bea-3a6c-4545-8daf-ac19ffcf248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)        \n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe7ab3",
   "metadata": {
    "papermill": {
     "duration": 0.017124,
     "end_time": "2021-04-04T11:45:57.276210",
     "exception": false,
     "start_time": "2021-04-04T11:45:57.259086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c59c351",
   "metadata": {
    "papermill": {
     "duration": 0.047922,
     "end_time": "2021-04-04T11:45:57.341390",
     "exception": false,
     "start_time": "2021-04-04T11:45:57.293468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, epoch, device=CFG['device']):\n",
    "    \n",
    "    dae.eval()\n",
    "    model.train()\n",
    "    final_loss = 0  \n",
    "    \n",
    "    noise_maker = SwapNoiseMasker(CFG['mlp_start_noise']*(CFG['mlp_noise_decay']**epoch))\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        \n",
    "        inputs, mask = noise_maker.apply(inputs)\n",
    "        \n",
    "        outputs = model(dae.feature(inputs)) #pass source data through DAE and MLP in one line\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()        \n",
    "        final_loss += loss.item()        \n",
    "    final_loss /= len(dataloader)    \n",
    "    return final_loss\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device=CFG['device']):\n",
    "    \n",
    "    dae.eval()\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(dae.feature(inputs))\n",
    "        loss = loss_fn(outputs, targets)        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device=CFG['device']):\n",
    "    \n",
    "    dae.eval()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(dae.feature(inputs))\n",
    "        #this predicts by BATCH requiring listing and concatenation\n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "    \n",
    "    #then we need to concatenate the list of batches\n",
    "    preds = np.concatenate(preds).reshape(-1,)  \n",
    "    return preds\n",
    "\n",
    "\n",
    "def run_training(X, y, len_train, len_test, folds, seed=42, batch_size=256, model_name='model',\n",
    "                 num_features=3000,\n",
    "                 num_targets=1,\n",
    "                 hidden_size=1000,\n",
    "                 device=CFG['device']):   \n",
    "    \n",
    "    print(len_train)\n",
    "    print(len_train+len_test)\n",
    "    \n",
    "    seed_everything(seed)    \n",
    "    \n",
    "    #placeholder - out of fold predictions\n",
    "    oof = np.zeros((len_train, ))\n",
    "    \n",
    "    #placeholder - test predictions\n",
    "    predictions = np.zeros((len_test, ))\n",
    "    \n",
    "    #placeholder - training/validation graph\n",
    "    fig,axes=plt.subplots(figsize=(18,6))\n",
    "    \n",
    "    #fold losses list\n",
    "    fold_losses = []\n",
    "    \n",
    "    for fold in sorted(np.unique(folds,return_counts=False)): \n",
    "        train_idx=folds[:len_train]!=fold\n",
    "        valid_idx=folds[:len_train]==fold\n",
    "            \n",
    "        print('     ')\n",
    "        print(f'training for fold {fold}')\n",
    "        #create the data set\n",
    "        train_dataset = FeatureDataset(X[:len_train][train_idx], y[:len_train][train_idx])\n",
    "        valid_dataset = FeatureDataset(X[:len_train][valid_idx], y[:len_train][valid_idx])\n",
    "\n",
    "        #apply to the data loader\n",
    "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)        \n",
    "        \n",
    "        #create the model itself\n",
    "        model = Model(\n",
    "            num_features=num_features,\n",
    "            num_targets=num_targets,\n",
    "            hidden_size=hidden_size,\n",
    "        )\n",
    "\n",
    "        #send to device and set up the loss and optimizer\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'],eps=0.00001 )\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                                  max_lr=1e-2, epochs=CFG['epochs'], steps_per_epoch=len(trainloader))\n",
    "\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        loss_tr = SmoothBCEwLogits(smoothing =CFG['bce_smooth'])\n",
    "\n",
    "        train_loss_list = []\n",
    "        valid_loss_list = []\n",
    "        best_loss = 9999999\n",
    "        for epoch in range(CFG['epochs']):\n",
    "            #the below updates the model and loss\n",
    "            train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader,epoch, device)            \n",
    "            train_loss_list+=[train_loss]\n",
    "            \n",
    "            #the below returns the validation predictions for the fold for each epoch\n",
    "            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, device)\n",
    "            valid_loss_list+=[valid_loss]            \n",
    "            \n",
    "            print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "            \n",
    "            if valid_loss < best_loss:\n",
    "                #continue training if improving\n",
    "                best_loss = valid_loss\n",
    "                oof[valid_idx] = valid_preds.reshape(-1,)\n",
    "                torch.save(model.state_dict(), SAVE_PATH + f\"MODEL_{CFG['model_name']}_FOLD{fold}_SEED{seed}.pth\")                    \n",
    "                    \n",
    "        fold_losses += [valid_loss_list[-1]]\n",
    "                    \n",
    "        del trainloader, validloader, train_dataset, valid_dataset\n",
    "        gc.collect()\n",
    "\n",
    "        sns.lineplot(x=range(CFG['epochs']), y=pd.Series(train_loss_list), color='Blue', ax=axes)\n",
    "        sns.lineplot(x=range(CFG['epochs']), y=pd.Series(valid_loss_list), color='Red', ax=axes)\n",
    "        \n",
    "        #--------------------- PREDICTION---------------------\n",
    "        #predict test data for fold\n",
    "        testdataset = TestFeatureDataset(X[len_train:len_train+len_test])\n",
    "        testloader = torch.utils.data.DataLoader(testdataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        #we create the model and then we input the latest weights\n",
    "        model = Model(\n",
    "            num_features=num_features,\n",
    "            num_targets=num_targets,\n",
    "            hidden_size=hidden_size,\n",
    "        )\n",
    "\n",
    "        model.load_state_dict(torch.load(SAVE_PATH + f\"MODEL_{CFG['model_name']}_FOLD{fold}_SEED{seed}.pth\"))\n",
    "        model.to(device)\n",
    "\n",
    "        #predictions need to be added for the fold\n",
    "        predictions += inference_fn(model, testloader, device)            \n",
    "    \n",
    "    print('finished with fold losses', fold_losses)\n",
    "    \n",
    "    predictions /= CFG['nfolds']\n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8d1b3",
   "metadata": {
    "papermill": {
     "duration": 0.017341,
     "end_time": "2021-04-04T11:45:57.376363",
     "exception": false,
     "start_time": "2021-04-04T11:45:57.359022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Data and Model and Create Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91a21d",
   "metadata": {
    "papermill": {
     "duration": 16.197888,
     "end_time": "2021-04-04T11:46:13.591566",
     "exception": false,
     "start_time": "2021-04-04T11:45:57.393678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  get data\n",
    "X, Y, n_cats, n_nums, unique_counts = get_data()\n",
    "\n",
    "# setup model\n",
    "dae = TransformerAutoEncoder(\n",
    "    num_inputs=X.shape[1],\n",
    "    n_cats=n_cats,\n",
    "    n_nums=n_nums,\n",
    "    **model_params\n",
    ").cuda()\n",
    "\n",
    "dae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e01438d",
   "metadata": {
    "papermill": {
     "duration": 0.104227,
     "end_time": "2021-04-04T11:46:13.716463",
     "exception": false,
     "start_time": "2021-04-04T11:46:13.612236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Using Simple Target Stratified KFold')\n",
    "train = pd.read_csv(PATH + \"queue_dataset_train_small_sample.csv\")\n",
    "test = pd.read_csv(PATH + \"queue_dataset_test.csv\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CFG['nfolds'], shuffle=True, random_state=CFG['random_state'])\n",
    "\n",
    "folds=np.zeros((len(X),)).astype(np.int32)\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X[:train.shape[0]], Y)):\n",
    "    folds[val_idx] = fold\n",
    "\n",
    "fold_values = sorted(np.unique(folds, return_counts=False))\n",
    "fold_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113f0cda",
   "metadata": {
    "papermill": {
     "duration": 0.019615,
     "end_time": "2021-04-04T11:46:13.755989",
     "exception": false,
     "start_time": "2021-04-04T11:46:13.736374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Weights and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a373847",
   "metadata": {
    "papermill": {
     "duration": 1649.909759,
     "end_time": "2021-04-04T12:13:43.684746",
     "exception": false,
     "start_time": "2021-04-04T11:46:13.774987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load weights and train\n",
    "model_state = torch.load(CFG['target_dae']+CFG['target_epoch'])\n",
    "dae.load_state_dict(model_state['model'])\n",
    "\n",
    "#note - I am not creating and saving features because the DAE will create the feature for each batch and immediately feed to MLP\n",
    "\n",
    "#run training\n",
    "# oof, predictions = run_training(X, Y.astype(np.long).reshape(-1), \n",
    "oof, predictions = run_training(X, Y, \n",
    "                                len_train=train.shape[0],\n",
    "                                len_test=test.shape[0],\n",
    "                                folds=folds,\n",
    "                                seed=CFG['random_state'],\n",
    "                                batch_size=CFG['mlp_batch_size'],\n",
    "                                model_name='model',\n",
    "                                num_features=num_dae_features+2*(np.array(unique_counts).sum()+len(fts_continuous)),\n",
    "                                num_targets=1,\n",
    "                                hidden_size=CFG['mlp_hidden_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c56d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "pd.read_csv(PATH + \"queue_dataset_train_small_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a07d4",
   "metadata": {
    "papermill": {
     "duration": 2.416217,
     "end_time": "2021-04-04T12:13:46.148524",
     "exception": false,
     "start_time": "2021-04-04T12:13:43.732307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#oof_df = pd.read_csv(PATH + 'train_data.csv')[['id', 'y']]\n",
    "oof_df = train[['id', TARGET]]\n",
    "\n",
    "oof_df['oof_prediction'] = np.where(oof>0.5, 1, 0)\n",
    "\n",
    "acc_score = accuracy_score(oof_df[TARGET], oof_df['oof_prediction'])\n",
    "print('CV Score', acc_score)\n",
    "\n",
    "oof_df.to_csv(f\"mlp_oof_cv{acc_score:.6f}.csv\", index=False)\n",
    "oof_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e67e3c1-5ecb-41d1-817a-4244cf5e8c37",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6f33f4-e258-4047-a00e-be81518e7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "cm = confusion_matrix(train[TARGET], np.where(oof>0.5, 1, 0))\n",
    "sns.heatmap(cm, annot=True, fmt='3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dac878-1c45-4f1d-abd6-d7f1ee357686",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270f0d4",
   "metadata": {
    "papermill": {
     "duration": 0.823261,
     "end_time": "2021-04-04T12:13:47.021793",
     "exception": false,
     "start_time": "2021-04-04T12:13:46.198532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(PATH + \"queue_dataset_test.csv\")\n",
    "submission[TARGET] = np.where(predictions>0.5, 1, 0)\n",
    "submission[['id', TARGET]].to_csv(f\"mlp_submission_cv{acc_score:.6f}.csv\", index=False)\n",
    "submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[TARGET].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea0b24c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1680.091991,
   "end_time": "2021-04-04T12:13:48.993371",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-04T11:45:48.901380",
   "version": "2.3.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}