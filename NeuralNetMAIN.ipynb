{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2Pdc8sQ0IvwrqSDFDfCKW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elvinaqa/HandsonML-and-DLTheory/blob/master/NeuralNetMAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AhrxD2WCMty",
        "colab_type": "text"
      },
      "source": [
        "### Supervised Learning\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> simpler continous value prediction\n",
        ">> Regression\n",
        "\n",
        ">> Classification\n",
        "\n",
        "### Unsupervised Learning\n",
        "> Clustering\n",
        "\n",
        "> Recommender Systems\n",
        "### Self-supervised Learning\n",
        "> Predict next frame in video\n",
        ">> Autoencoder in NN\n",
        "### Reinforcement Learning\n",
        "> AI plays game\n",
        ">> Markov Decision Learning\n",
        "\n",
        ">> Q Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cpkwfmeCWp1",
        "colab_type": "text"
      },
      "source": [
        "## Specific Topic Notes\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Forward Propogation\n",
        "> a forward run through neurons in the training process\n",
        "\n",
        "### Activation Function\n",
        "A function which squahes the output value of neuron to ex: 0, 1 in Sigmoid, or to positive values in ReLu\n",
        "\n",
        "### Loss Function\n",
        "> MSE - error from the actual weight for neuron in each run\n",
        "\n",
        "### Back propogation - Chain Rule\n",
        "> dy/dx = (dy/du) * (du/dx)\n",
        "> Revisit neurons to fix and correct weight - gradients after each forward run. Reduce the error and loss rates.\n",
        "\n",
        "### Gradient Descent\n",
        "> optimizing the loss and error rates to the lower levels in the process of back propogation, while adjusting weights.\n",
        "> Mini Batch Gradient Descent which performs and optimize gradients for each batch process\n",
        "\n",
        "### Learning Rate\n",
        "> In traverse for finding minima, LR is a jump, while it can be small, larger values speeds up training process and increases\n",
        "#### the chance of over-shooting the minimum point\n",
        "\n",
        "\n",
        "### Overfitting\n",
        "> Perfectly fitting to train, while poor performance on test data\n",
        "\n",
        "### Regularization\n",
        "A method used to avoid overfitting\n",
        "> L1, L2 - while cross validation and other methods work as well to prevent overfitting for smaller datasets, Lasso, and Ridge works better for larger datasets\n",
        "\n",
        "> L1 - Lasso\n",
        ">> L1 brings the unimportant feature weights to zero, removes features and applies feature selection\n",
        "\n",
        "> L2 - Ridge\n",
        ">> Penalize large weights quadratically\n",
        "\n",
        "> Cross Validation\n",
        ">> K-fold is a method used to prevent overfitting by diving and splitting the training set into k folds. Let's say we split the data into 5 folds, train on 4 of them and keep the last fold for testing. Out of 4 folds, we train the model on each and average the weights coming from each fold and its cycle.\n",
        "While cross validation reduces overfitting, it slows down the training process.\n",
        "\n",
        "> Early Stopping\n",
        ">> Early stopping the training process when the loss is already minimum before finishing up the epochs previously predefined \n",
        "\n",
        "> Dropout\n",
        ">> Dropping some \"harmful\" neurons during the training process, for forward and backward runs in the aim for reducing overfitting\n",
        "\n",
        "> Data Augmentation \n",
        ">> Augmentation is to improve the amount of data by making variations so that model do not overfit to the smaller bunch of train data\n",
        "\n",
        "### Epoch \n",
        "> Full dataset is forward and backward propogated through the training process\n",
        "\n",
        "### Batch\n",
        "> Number of samples from the training dataset's one epoch depending on the RAM, it can be bigger or smaller in terms of size\n",
        "\n",
        "### Iterations\n",
        "> Number of batches to complete one epoch in the proces. 10 iterations needed to complete 1000 samples of data for completion of one epoch\n",
        "\n",
        "### Confusion Matrix\n",
        "Loss and Accuracy are completely correlated\n",
        "\n",
        "> Loss\n",
        "\n",
        "> Accuracy - Correct predictions out of all predictions\n",
        "\n",
        "\n",
        ">> True Positive - positive + , True\n",
        ">> True Negative - negative - , True\n",
        ">> False Positive - negative - , False\n",
        ">> False Negative - positive + , False\n",
        "\n",
        "### Recall\n",
        "How many times result is positive and prediction is True\n",
        "> TP/TP + FN\n",
        "\n",
        "### Precision\n",
        "How many times positive and negative predictions are True\n",
        "> TP/TP + FP\n",
        "\n",
        "### F1 score\n",
        "A score showing combination of Recall and Precision\n",
        "> 2* Recall*Precision/ Recall + Precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDI6E72FF1Hm",
        "colab_type": "text"
      },
      "source": [
        "## Best ML Practice\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Activation Function\n",
        "> ReLu\n",
        "\n",
        "> Leaky ReLu\n",
        "\n",
        "### Loss Function\n",
        "> MSE\n",
        "\n",
        "### Regularization\n",
        "> L2, start small: 0.01\n",
        "\n",
        "> Dropout, set P 0.2 - 0.5\n",
        "\n",
        "> Data Augmentation for CNN\n",
        "### Learning Rate\n",
        "> 0.001\n",
        "\n",
        "### Hidden Layers\n",
        "> As PC performance let's\n",
        "\n",
        "### Epochs\n",
        "> Usually 10-100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i19gmZMkIUn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}